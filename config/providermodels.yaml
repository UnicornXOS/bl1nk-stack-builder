# Provider models configuration
# Maps specific models to providers with detailed specifications

# =============================================================================
# LLM MODELS CONFIGURATION
# =============================================================================

llm_models:
  # OpenRouter models
  openrouter:
    # Claude models
    claude-3-opus:
      name: "claude-3-opus-20240229"
      display_name: "Claude 3 Opus"
      description: "Most intelligent model, excellent for complex reasoning"
      provider: "openrouter"
      context_window: 200000
      max_tokens: 4096
      cost_per_1k_input: 0.075
      cost_per_1k_output: 0.150
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.95
      speed_score: 0.7
      cost_score: 0.3
      use_cases:
        - "complex_reasoning"
        - "analysis"
        - "writing"
        - "coding"

    claude-3-sonnet:
      name: "claude-3-sonnet-20240229"
      display_name: "Claude 3 Sonnet"
      description: "Balanced model for most tasks"
      provider: "openrouter"
      context_window: 200000
      max_tokens: 4096
      cost_per_1k_input: 0.015
      cost_per_1k_output: 0.075
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.90
      speed_score: 0.85
      cost_score: 0.7
      use_cases:
        - "general_chat"
        - "summarization"
        - "qa"
        - "coding"

    claude-3-haiku:
      name: "claude-3-haiku-20240307"
      display_name: "Claude 3 Haiku"
      description: "Fastest model for simple tasks"
      provider: "openrouter"
      context_window: 200000
      max_tokens: 4096
      cost_per_1k_input: 0.005
      cost_per_1k_output: 0.025
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.80
      speed_score: 0.95
      cost_score: 0.9
      use_cases:
        - "quick_responses"
        - "simple_qa"
        - "summarization"

    # GPT models
    gpt-4-turbo:
      name: "gpt-4-turbo"
      display_name: "GPT-4 Turbo"
      description: "Latest GPT-4 model with improved performance"
      provider: "openrouter"
      context_window: 128000
      max_tokens: 4096
      cost_per_1k_input: 0.030
      cost_per_1k_output: 0.090
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.92
      speed_score: 0.80
      cost_score: 0.6
      use_cases:
        - "general_chat"
        - "coding"
        - "analysis"
        - "writing"

    gpt-4:
      name: "gpt-4"
      display_name: "GPT-4"
      description: "Original GPT-4 model"
      provider: "openrouter"
      context_window: 8192
      max_tokens: 4096
      cost_per_1k_input: 0.060
      cost_per_1k_output: 0.120
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.90
      speed_score: 0.75
      cost_score: 0.4
      use_cases:
        - "complex_reasoning"
        - "coding"
        - "analysis"

    # Llama models
    llama-3.1-405b-instruct:
      name: "llama-3.1-405b-instruct"
      display_name: "Llama 3.1 405B"
      description: "Meta's largest and most capable model"
      provider: "openrouter"
      context_window: 131072
      max_tokens: 4096
      cost_per_1k_input: 0.003
      cost_per_1k_output: 0.003
      supports_streaming: true
      supports_function_calling: false
      supports_vision: false
      quality_score: 0.85
      speed_score: 0.6
      cost_score: 0.9
      use_cases:
        - "general_chat"
        - "summarization"
        - "reasoning"

    llama-3.1-70b-instruct:
      name: "llama-3.1-70b-instruct"
      display_name: "Llama 3.1 70B"
      description: "High-performance open model"
      provider: "openrouter"
      context_window: 131072
      max_tokens: 4096
      cost_per_1k_input: 0.001
      cost_per_1k_output: 0.001
      supports_streaming: true
      supports_function_calling: false
      supports_vision: false
      quality_score: 0.82
      speed_score: 0.8
      cost_score: 0.95
      use_cases:
        - "general_chat"
        - "summarization"
        - "reasoning"

    llama-3.1-8b-instruct:
      name: "llama-3.1-8b-instruct"
      display_name: "Llama 3.1 8B"
      description: "Fast and efficient open model"
      provider: "openrouter"
      context_window: 131072
      max_tokens: 4096
      cost_per_1k_input: 0.0004
      cost_per_1k_output: 0.0004
      supports_streaming: true
      supports_function_calling: false
      supports_vision: false
      quality_score: 0.75
      speed_score: 0.95
      cost_score: 1.0
      use_cases:
        - "quick_responses"
        - "simple_chat"
        - "summarization"

  # Cloudflare models
  cloudflare:
    meta-llama-3.1-70b-instruct:
      name: "@cf/meta/llama-3.1-70b-instruct"
      display_name: "Llama 3.1 70B (Cloudflare)"
      description: "Meta Llama model hosted on Cloudflare"
      provider: "cloudflare"
      context_window: 131072
      max_tokens: 4096
      cost_per_1k_input: 0.002
      cost_per_1k_output: 0.002
      supports_streaming: true
      supports_function_calling: false
      supports_vision: false
      quality_score: 0.82
      speed_score: 0.85
      cost_score: 0.9
      use_cases:
        - "general_chat"
        - "summarization"

    meta-llama-3.1-8b-instruct:
      name: "@cf/meta/llama-3.1-8b-instruct"
      display_name: "Llama 3.1 8B (Cloudflare)"
      description: "Fast Meta Llama model on Cloudflare"
      provider: "cloudflare"
      context_window: 131072
      max_tokens: 4096
      cost_per_1k_input: 0.0004
      cost_per_1k_output: 0.0004
      supports_streaming: true
      supports_function_calling: false
      supports_vision: false
      quality_score: 0.75
      speed_score: 0.95
      cost_score: 1.0
      use_cases:
        - "quick_responses"
        - "simple_chat"

  # AWS Bedrock models
  bedrock:
    claude-3-5-sonnet:
      name: "anthropic.claude-3-5-sonnet-20241022-v1:0"
      display_name: "Claude 3.5 Sonnet (Bedrock)"
      description: "Anthropic Claude 3.5 Sonnet via AWS Bedrock"
      provider: "bedrock"
      context_window: 200000
      max_tokens: 4096
      cost_per_1k_input: 0.015
      cost_per_1k_output: 0.075
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.90
      speed_score: 0.85
      cost_score: 0.7
      use_cases:
        - "general_chat"
        - "coding"
        - "analysis"

    claude-3-haiku:
      name: "anthropic.claude-3-haiku-20240307-v1:0"
      display_name: "Claude 3 Haiku (Bedrock)"
      description: "Fast Anthropic model via AWS Bedrock"
      provider: "bedrock"
      context_window: 200000
      max_tokens: 4096
      cost_per_1k_input: 0.005
      cost_per_1k_output: 0.025
      supports_streaming: true
      supports_function_calling: true
      supports_vision: true
      quality_score: 0.80
      speed_score: 0.95
      cost_score: 0.9
      use_cases:
        - "quick_responses"
        - "simple_qa"

    llama-2-70b-chat:
      name: "meta.llama2-70b-chat-v1"
      display_name: "Llama 2 70B Chat (Bedrock)"
      description: "Meta Llama 2 chat model via AWS Bedrock"
      provider: "bedrock"
      context_window: 4096
      max_tokens: 4096
      cost_per_1k_input: 0.001
      cost_per_1k_output: 0.001
      supports_streaming: true
      supports_function_calling: false
      supports_vision: false
      quality_score: 0.75
      speed_score: 0.7
      cost_score: 0.95
      use_cases:
        - "general_chat"
        - "reasoning"

# =============================================================================
# EMBEDDING MODELS CONFIGURATION
# =============================================================================

embedding_models:
  openrouter:
    gamma-300:
      name: "gamma-300"
      display_name: "Gamma 300"
      description: "Fast and cost-effective embedding model"
      provider: "openrouter"
      vector_dim: 768
      max_input_length: 8000
      cost_per_1k_tokens: 0.0001
      supports_batch: true
      batch_size: 100
      quality_score: 0.80
      speed_score: 0.95
      cost_score: 1.0
      use_cases:
        - "general_embeddings"
        - "semantic_search"
        - "clustering"

    text-embedding-3-large:
      name: "text-embedding-3-large"
      display_name: "OpenAI text-embedding-3-large"
      description: "High-quality large embedding model"
      provider: "openrouter"
      vector_dim: 3072
      max_input_length: 8000
      cost_per_1k_tokens: 0.00013
      supports_batch: true
      batch_size: 100
      quality_score: 0.95
      speed_score: 0.7
      cost_score: 0.8
      use_cases:
        - "high_quality_search"
        - "fine_grained_clustering"
        - "semantic_similarity"

    text-embedding-3-small:
      name: "text-embedding-3-small"
      display_name: "OpenAI text-embedding-3-small"
      description: "Fast and cost-effective small embedding model"
      provider: "openrouter"
      vector_dim: 1536
      max_input_length: 8000
      cost_per_1k_tokens: 0.00002
      supports_batch: true
      batch_size: 100
      quality_score: 0.85
      speed_score: 0.95
      cost_score: 1.0
      use_cases:
        - "general_embeddings"
        - "semantic_search"
        - "large_scale_indexing"

  cohere:
    embed-multilingual-v3.0:
      name: "embed-multilingual-v3.0"
      display_name: "Cohere Embed Multilingual v3"
      description: "Multilingual embedding model"
      provider: "cohere"
      vector_dim: 1024
      max_input_length: 5000
      cost_per_1k_tokens: 0.0001
      supports_batch: true
      batch_size: 96
      quality_score: 0.88
      speed_score: 0.8
      cost_score: 0.9
      use_cases:
        - "multilingual_search"
        - "cross_language_similarity"

# =============================================================================
# RERANKING MODELS CONFIGURATION
# =============================================================================

rerank_models:
  openrouter:
    voyage_ai:
      name: "voyage-ai"
      display_name: "Voyage AI Reranker"
      description: "State-of-the-art reranking model"
      provider: "openrouter"
      max_input_length: 32000
      cost_per_1k_tokens: 0.001
      quality_score: 0.95
      speed_score: 0.7
      cost_score: 0.8
      use_cases:
        - "semantic_reranking"
        - "search_result_reranking"

    bge-reranker-base:
      name: "bge-reranker-base"
      display_name: "BGE Reranker Base"
      description: "Base reranking model"
      provider: "openrouter"
      max_input_length: 512
      cost_per_1k_tokens: 0.0005
      quality_score: 0.85
      speed_score: 0.9
      cost_score: 0.9
      use_cases:
        - "lightweight_reranking"
        - "real_time_reranking"

# =============================================================================
# MODEL ROUTING RULES
# =============================================================================

routing_rules:
  # Task type to model mapping
  task_mappings:
    general_chat:
      - claude-3-sonnet
      - llama-3.1-70b-instruct
      - @cf/meta/llama-3.1-70b-instruct
    
    coding:
      - claude-3-opus
      - gpt-4-turbo
      - claude-3-sonnet
    
    analysis:
      - claude-3-opus
      - gpt-4-turbo
      - claude-3-sonnet
    
    writing:
      - claude-3-opus
      - claude-3-sonnet
      - gpt-4-turbo
    
    summarization:
      - claude-3-haiku
      - llama-3.1-8b-instruct
      - @cf/meta/llama-3.1-8b-instruct
    
    quick_responses:
      - claude-3-haiku
      - llama-3.1-8b-instruct
      - @cf/meta/llama-3.1-8b-instruct

  # User tier to model mapping
  tier_mappings:
    free:
      - llama-3.1-8b-instruct
      - @cf/meta/llama-3.1-8b-instruct
      - claude-3-haiku
    
    pro:
      - claude-3-sonnet
      - llama-3.1-70b-instruct
      - @cf/meta/llama-3.1-70b-instruct
    
    ultimate:
      - claude-3-opus
      - gpt-4-turbo
      - claude-3-5-sonnet

  # Context length requirements
  context_requirements:
    short_context:
      max_length: 4096
      preferred_models:
        - llama-2-70b-chat
        - gpt-4
    
    medium_context:
      max_length: 32768
      preferred_models:
        - llama-3.1-70b-instruct
        - @cf/meta/llama-3.1-70b-instruct
        - gpt-4-turbo
    
    long_context:
      max_length: 131072
      preferred_models:
        - claude-3-sonnet
        - claude-3-haiku
        - llama-3.1-405b-instruct
    
    extended_context:
      max_length: 200000
      preferred_models:
        - claude-3-opus
        - claude-3-sonnet
        - claude-3-haiku

  # Feature requirements
  feature_mappings:
    vision_required:
      - claude-3-opus
      - claude-3-sonnet
      - claude-3-haiku
      - gpt-4-turbo
    
    function_calling_required:
      - claude-3-opus
      - claude-3-sonnet
      - claude-3-haiku
      - gpt-4-turbo
    
    streaming_required:
      - claude-3-opus
      - claude-3-sonnet
      - claude-3-haiku
      - gpt-4-turbo
      - llama-3.1-70b-instruct
      - llama-3.1-8b-instruct

# =============================================================================
# MODEL EVALUATION METRICS
# =============================================================================

evaluation_metrics:
  # Quality metrics (0-1 scale)
  quality_metrics:
    factual_accuracy: 0.9
    coherence: 0.85
    creativity: 0.8
    instruction_following: 0.9
    reasoning: 0.85
  
  # Performance metrics
  performance_metrics:
    latency_p50_ms: 2000
    latency_p95_ms: 5000
    latency_p99_ms: 10000
    throughput_tokens_per_second: 50
    success_rate: 0.95
  
  # Cost metrics
  cost_metrics:
    cost_per_1k_tokens_input: 0.01
    cost_per_1k_tokens_output: 0.03
    cost_per_request: 0.05
    efficiency_score: 0.8

# =============================================================================
# MODEL DEPRECATION AND MIGRATION
# =============================================================================

model_lifecycle:
  # Model deprecation schedule
  deprecated_models:
    - model: "gpt-3.5-turbo"
      deprecated_date: "2024-09-01"
      replacement: "gpt-4-turbo"
      migration_period_days: 90
  
  # Model version compatibility
  version_compatibility:
    "claude-3": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"]
    "llama-3.1": ["llama-3.1-405b-instruct", "llama-3.1-70b-instruct", "llama-3.1-8b-instruct"]
    "gpt-4": ["gpt-4-turbo", "gpt-4"]

  # Auto-migration rules
  auto_migration:
    enabled: true
    conditions:
      - model_deprecated: true
      - error_rate_increase: 0.1
      - cost_increase: 2.0
    fallback_models:
      - claude-3-sonnet
      - llama-3.1-70b-instruct