# Provider routing configuration for bl1nk-agent-builder
# This file defines provider priorities, fallback logic, and cost controls

providers:
  # OpenRouter - Primary provider
  openrouter:
    priority: 1
    base_url: "https://openrouter.ai/api/v1"
    timeout_seconds: 30
    retry_policy:
      max_retries: 3
      backoff_base: 0.5
      backoff_factor: 2.0
      max_backoff: 30.0
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 100000
      burst_requests: 10
    cost_control:
      enabled: true
      max_cost_per_request: 0.10
      max_daily_cost: 10.00
      max_monthly_cost: 100.00
    health_check:
      enabled: true
      interval_seconds: 300
      timeout_seconds: 5
      failure_threshold: 3
      success_threshold: 2
    models:
      - name: "claude-3.5-sonnet"
        cost_per_1k_tokens_input: 0.015
        cost_per_1k_tokens_output: 0.075
        max_tokens: 200000
        context_window: 200000
        supports_streaming: true
        supports_function_calling: true
      - name: "claude-3-haiku"
        cost_per_1k_tokens_input: 0.005
        cost_per_1k_tokens_output: 0.025
        max_tokens: 200000
        context_window: 200000
        supports_streaming: true
        supports_function_calling: true
      - name: "gpt-4-turbo"
        cost_per_1k_tokens_input: 0.030
        cost_per_1k_tokens_output: 0.090
        max_tokens: 128000
        context_window: 128000
        supports_streaming: true
        supports_function_calling: true
      - name: "llama-3.1-70b-instruct"
        cost_per_1k_tokens_input: 0.001
        cost_per_1k_tokens_output: 0.001
        max_tokens: 131072
        context_window: 131072
        supports_streaming: true
        supports_function_calling: false

  # Cloudflare Gateway - Secondary provider
  cloudflare:
    priority: 2
    base_url: "https://gateway.ai.cloudflare.com"
    timeout_seconds: 30
    retry_policy:
      max_retries: 2
      backoff_base: 1.0
      backoff_factor: 2.0
      max_backoff: 30.0
    rate_limit:
      requests_per_minute: 30
      tokens_per_minute: 50000
      burst_requests: 5
    cost_control:
      enabled: true
      max_cost_per_request: 0.15
      max_daily_cost: 15.00
      max_monthly_cost: 150.00
    health_check:
      enabled: true
      interval_seconds: 300
      timeout_seconds: 5
      failure_threshold: 3
      success_threshold: 2
    models:
      - name: "@cf/meta/llama-3.1-70b-instruct"
        cost_per_1k_tokens_input: 0.002
        cost_per_1k_tokens_output: 0.002
        max_tokens: 131072
        context_window: 131072
        supports_streaming: true
        supports_function_calling: false
      - name: "@cf/meta/llama-3.1-8b-instruct"
        cost_per_1k_tokens_input: 0.0004
        cost_per_1k_tokens_output: 0.0004
        max_tokens: 131072
        context_window: 131072
        supports_streaming: true
        supports_function_calling: false

  # AWS Bedrock - Tertiary provider (fallback)
  bedrock:
    priority: 3
    region: "us-east-1"
    timeout_seconds: 60
    retry_policy:
      max_retries: 5
      backoff_base: 0.5
      backoff_factor: 2.0
      max_backoff: 60.0
    rate_limit:
      requests_per_minute: 20
      tokens_per_minute: 40000
      burst_requests: 3
    cost_control:
      enabled: true
      max_cost_per_request: 0.20
      max_daily_cost: 20.00
      max_monthly_cost: 200.00
    health_check:
      enabled: true
      interval_seconds: 300
      timeout_seconds: 10
      failure_threshold: 3
      success_threshold: 2
    models:
      - name: "anthropic.claude-3-5-sonnet-20241022-v1:0"
        cost_per_1k_tokens_input: 0.015
        cost_per_1k_tokens_output: 0.075
        max_tokens: 200000
        context_window: 200000
        supports_streaming: true
        supports_function_calling: true
      - name: "anthropic.claude-3-haiku-20240307-v1:0"
        cost_per_1k_tokens_input: 0.005
        cost_per_1k_tokens_output: 0.025
        max_tokens: 200000
        context_window: 200000
        supports_streaming: true
        supports_function_calling: true
      - name: "meta.llama2-70b-chat-v1"
        cost_per_1k_tokens_input: 0.001
        cost_per_1k_tokens_output: 0.001
        max_tokens: 4096
        context_window: 4096
        supports_streaming: true
        supports_function_calling: false

# Default model configurations
models:
  # Embedding models
  embedding:
    default: "gamma-300"
    provider: "openrouter"
    vector_dim: 768
    cache_ttl_seconds: 86400
    models:
      - name: "gamma-300"
        provider: "openrouter"
        cost_per_1k_tokens: 0.0001
        vector_dim: 768
        max_input_length: 8000
      - name: "text-embedding-3-large"
        provider: "openrouter"
        cost_per_1k_tokens: 0.00013
        vector_dim: 3072
        max_input_length: 8000
      - name: "text-embedding-3-small"
        provider: "openrouter"
        cost_per_1k_tokens: 0.00002
        vector_dim: 1536
        max_input_length: 8000

  # Generation models by tier
  generation:
    # Budget/tier users
    cheap:
      - model: "llama-3.1-8b-instruct"
        provider: "cloudflare"
        cost_per_1k_tokens: 0.001
        max_tokens: 131072
        context_window: 131072
        use_case: "general_chat"
      - model: "llama-3.1-70b-instruct"
        provider: "bedrock"
        cost_per_1k_tokens: 0.001
        max_tokens: 131072
        context_window: 131072
        use_case: "general_chat"

    # Standard/pro tier users
    standard:
      - model: "claude-3-haiku"
        provider: "openrouter"
        cost_per_1k_tokens: 0.015
        max_tokens: 200000
        context_window: 200000
        use_case: "general_chat"
        supports_function_calling: true
      - model: "@cf/meta/llama-3.1-70b-instruct"
        provider: "cloudflare"
        cost_per_1k_tokens: 0.002
        max_tokens: 131072
        context_window: 131072
        use_case: "general_chat"

    # Premium/ultimate tier users
    premium:
      - model: "claude-3.5-sonnet"
        provider: "openrouter"
        cost_per_1k_tokens: 0.045
        max_tokens: 200000
        context_window: 200000
        use_case: "advanced_chat"
        supports_function_calling: true
        supports_vision: true
      - model: "anthropic.claude-3-5-sonnet-20241022-v1:0"
        provider: "bedrock"
        cost_per_1k_tokens: 0.045
        max_tokens: 200000
        context_window: 200000
        use_case: "advanced_chat"
        supports_function_calling: true
        supports_vision: true

    # Fallback models (if all else fails)
    fallback:
      - model: "llama-2-7b-chat"
        provider: "bedrock"
        cost_per_1k_tokens: 0.001
        max_tokens: 4096
        context_window: 4096
        use_case: "emergency"

  # Reranking models
  rerank:
    default: "voyage_ai"
    provider: "openrouter"
    cost_per_1k_tokens: 0.001
    models:
      - name: "voyage_ai"
        provider: "openrouter"
        cost_per_1k_tokens: 0.001
        max_input_length: 32000
      - name: "bge-reranker-base"
        provider: "openrouter"
        cost_per_1k_tokens: 0.0005
        max_input_length: 512

# Cost control settings
cost_control:
  # Global limits
  default_threshold_usd: 0.50
  emergency_stop_threshold_usd: 10.00
  
  # Tier-based limits
  tier_limits:
    free:
      monthly_budget_usd: 10.00
      per_task_max_usd: 0.10
      daily_requests_limit: 100
      hourly_requests_limit: 20
      concurrent_tasks_limit: 2
    pro:
      monthly_budget_usd: 500.00
      per_task_max_usd: 5.00
      daily_requests_limit: 1000
      hourly_requests_limit: 200
      concurrent_tasks_limit: 10
    ultimate:
      monthly_budget_usd: null
      per_task_max_usd: null
      daily_requests_limit: null
      hourly_requests_limit: null
      concurrent_tasks_limit: null
  
  # Per-provider limits
  provider_limits:
    openrouter:
      max_daily_spend: 50.00
      max_requests_per_hour: 1000
    cloudflare:
      max_daily_spend: 30.00
      max_requests_per_hour: 500
    bedrock:
      max_daily_spend: 100.00
      max_requests_per_hour: 200

# Routing strategies
routing_strategies:
  # Round-robin for load balancing
  round_robin:
    enabled: false
    weight_providers: false
  
  # Cost-optimized routing
  cost_optimized:
    enabled: true
    prefer_cheaper_models: true
    budget_aware: true
  
  # Performance-optimized routing
  performance_optimized:
    enabled: false
    prefer_faster_models: true
    latency_aware: true
  
  # Quality-optimized routing
  quality_optimized:
    enabled: false
    prefer_high_quality_models: true
    quality_threshold: 0.8

# Health monitoring
health_monitoring:
  enabled: true
  check_interval_seconds: 60
  failure_threshold: 3
  recovery_threshold: 2
  
  # Provider health endpoints
  health_endpoints:
    openrouter: "https://openrouter.ai/api/v1/models"
    cloudflare: "https://gateway.ai.cloudflare.com/v1"
    bedrock: "https://bedrock-runtime.us-east-1.amazonaws.com"
  
  # Alert thresholds
  alert_thresholds:
    error_rate: 0.05  # 5% error rate
    latency_p95: 30   # 30 seconds P95 latency
    cost_spike: 2.0   # 2x normal cost

# Failover configuration
failover:
  enabled: true
  automatic: true
  
  # Failover triggers
  triggers:
    - error_rate_threshold: 0.10
    - timeout_threshold: 3
    - cost_exceeded: true
    - health_check_failed: true
  
  # Recovery conditions
  recovery:
    success_rate_threshold: 0.95
    consecutive_successes: 5
    time_window_minutes: 10

# Monitoring and alerts
monitoring:
  enabled: true
  
  # Metrics to track
  metrics:
    - request_count
    - error_rate
    - latency_p50
    - latency_p95
    - latency_p99
    - cost_per_request
    - cost_per_hour
    - cost_per_day
    - token_usage
    - active_sessions
  
  # Alert rules
  alerts:
    - name: "High Error Rate"
      condition: "error_rate > 0.05"
      severity: "warning"
      actions: ["slack", "email"]
    - name: "Cost Spike"
      condition: "cost_per_hour > normal_cost * 2"
      severity: "critical"
      actions: ["slack", "email", "auto_disable"]
    - name: "Provider Down"
      condition: "provider_health == false"
      severity: "critical"
      actions: ["auto_failover", "slack"]